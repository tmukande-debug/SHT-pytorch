{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Wl3rGtsadZX9",
        "Hy3exoGgdnyF"
      ],
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# params.py"
      ],
      "metadata": {
        "id": "HHKBxZCSJREx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "\n",
        "def parse_args():\n",
        "\tparser = argparse.ArgumentParser(description='Model Params')\n",
        "\tparser.add_argument('--lr', default=1.5e-3, type=float, help='learning rate')\n",
        "\tparser.add_argument('--batch', default=256, type=int, help='batch size')\n",
        "\tparser.add_argument('--reg', default=3e-2, type=float, help='weight decay regularizer')\n",
        "\tparser.add_argument('--epoch', default=200, type=int, help='number of epochs')\n",
        "\tparser.add_argument('--decay', default=0.96, type=float, help='weight decay rate')\n",
        "\tparser.add_argument('--save_path', default='tem', help='file name to save model and training record')\n",
        "\tparser.add_argument('--latdim', default=32, type=int, help='embedding size')\n",
        "\tparser.add_argument('--rank', default=4, type=int, help='embedding size')\n",
        "\tparser.add_argument('--memosize', default=2, type=int, help='memory size')\n",
        "\tparser.add_argument('--sampNum', default=40, type=int, help='batch size for sampling')\n",
        "\tparser.add_argument('--att_head', default=2, type=int, help='number of attention heads')\n",
        "\tparser.add_argument('--gnn_layer', default=2, type=int, help='number of gnn layers')\n",
        "\tparser.add_argument('--hyperNum', default=128, type=int, help='number of hyper edges')\n",
        "\tparser.add_argument('--trnNum', default=10000, type=int, help='number of training instances per epoch')\n",
        "\tparser.add_argument('--load_model', default=None, help='model name to load')\n",
        "\tparser.add_argument('--shoot', default=20, type=int, help='K of top k')\n",
        "\tparser.add_argument('--data', default='yelp', type=str, help='name of dataset')\n",
        "\tparser.add_argument('--mult', default=100, type=float, help='multiplier for the result')\n",
        "\tparser.add_argument('--keepRate', default=0.5, type=float, help='rate for dropout')\n",
        "\tparser.add_argument('--slot', default=5, type=float, help='length of time slots')\n",
        "\tparser.add_argument('--divSize', default=10000, type=int, help='div size for smallTestEpoch')\n",
        "\tparser.add_argument('--tstEpoch', default=10, type=int, help='number of epoch to test while training')\n",
        "\tparser.add_argument('--leaky', default=0.5, type=float, help='slope for leaky relu')\n",
        "\tparser.add_argument('--gcn_hops', default=2, type=int, help='number of hops in gcn precessing')\n",
        "\tparser.add_argument('--ssl_reg', default=1e-4, type=float, help='reg weight for ssl loss')\n",
        "\tparser.add_argument('--edgeSampRate', default=0.5, type=float, help='Ratio of sampled edges')\n",
        "\treturn parser\n",
        "args, _ = parse_args().parse_known_args()\n",
        "args.decay_step = args.trnNum//args.batch\n",
        "if torch.cuda.is_available():\n",
        "\targs.device = \"cuda\"\n",
        "else:\n",
        "\targs.device = \"cpu\""
      ],
      "metadata": {
        "id": "TihmI5mTJOAK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader.py"
      ],
      "metadata": {
        "id": "8lyPXPf1dMNt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PqeCHqhecPyR"
      },
      "outputs": [],
      "source": [
        "import scipy.sparse as sp\n",
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "#from Params import args\n",
        "\n",
        "def transpose(mat):\n",
        "  coomat = sp.coo_matrix(mat)\n",
        "  return csr_matrix(coomat.transpose())\n",
        "\n",
        "def negSamp(temLabel, sampSize, nodeNum):\n",
        "  negset = [None] * sampSize\n",
        "  cur = 0\n",
        "  while cur < sampSize:\n",
        "    rdmItm = np.random.choice(nodeNum)\n",
        "    if temLabel[rdmItm] == 0:\n",
        "      negset[cur] = rdmItm\n",
        "      cur += 1\n",
        "  return negset\n",
        "\n",
        "def transToLsts(mat, mask=False, norm=False):\n",
        "  shape = torch.Size(mat.shape)\n",
        "  mat = sp.coo_matrix(mat)\n",
        "  indices = torch.from_numpy(np.vstack((mat.row, mat.col)).astype(np.int64))\n",
        "  data = mat.data\n",
        "  \n",
        "  if norm:\n",
        "    rowD = np.squeeze(np.array(1 / (np.sqrt(np.sum(mat, axis=1) + 1e-8) + 1e-8)))\n",
        "    colD = np.squeeze(np.array(1 / (np.sqrt(np.sum(mat, axis=0) + 1e-8) + 1e-8)))\n",
        "    for i in range(len(mat.data)):\n",
        "      row = indices[0, i]\n",
        "      col = indices[1, i]\n",
        "      data[i] = data[i] * rowD[row] * colD[col]\n",
        "  # half mask\n",
        "  if mask:\n",
        "    spMask = (np.random.uniform(size=data.shape) > 0.5) * 1.0\n",
        "    data = data * spMask\n",
        "\n",
        "  if indices.shape[0] == 0:\n",
        "    indices = np.array([[0, 0]], dtype=np.int32)\n",
        "    data = np.array([0.0], np.float32)\n",
        "\n",
        "  data = torch.from_numpy(data)\n",
        "  #a =torch.sparse.FloatTensor(indices, values, shape).to(torch.float32).cuda()\n",
        "  return indices, data, shape\n",
        "\n",
        "class DataHandler:\n",
        "  def __init__(self):\n",
        "    if args.data == 'yelp':\n",
        "      predir = '/content/drive/MyDrive/SHT/yelp/'\n",
        "    elif args.data == 'tmall':\n",
        "      predir = '/content/drive/MyDrive/SHT/tmall/'\n",
        "    elif args.data == 'gowalla':\n",
        "      predir = '/content/drive/MyDrive/SHT/gowalla/'\n",
        "    self.predir = predir\n",
        "    self.trnfile = predir + 'trnMat.pkl'\n",
        "    self.tstfile = predir + 'tstMat.pkl'\n",
        "\n",
        "  def LoadData(self):\n",
        "    with open(self.trnfile, 'rb') as fs:\n",
        "      trnMat = (pickle.load(fs) != 0).astype(np.float32)\n",
        "    # test set\n",
        "    with open(self.tstfile, 'rb') as fs:\n",
        "      tstMat = pickle.load(fs)\n",
        "    tstLocs = [None] * tstMat.shape[0]\n",
        "    tstUsrs = set()\n",
        "    for i in range(len(tstMat.data)):\n",
        "      row = tstMat.row[i]\n",
        "      col = tstMat.col[i]\n",
        "      if tstLocs[row] is None:\n",
        "        tstLocs[row] = list()\n",
        "      tstLocs[row].append(col)\n",
        "      tstUsrs.add(row)\n",
        "    tstUsrs = np.array(list(tstUsrs))\n",
        "\n",
        "    self.trnMat = trnMat\n",
        "    self.tstLocs = tstLocs\n",
        "    self.tstUsrs = tstUsrs\n",
        "    args.edgeNum = len(trnMat.data)\n",
        "    args.user, args.item = self.trnMat.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TimeLogger.py"
      ],
      "metadata": {
        "id": "Wl3rGtsadZX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "logmsg = ''\n",
        "timemark = dict()\n",
        "saveDefault = False\n",
        "def log(msg, save=None, oneline=False):\n",
        "\tglobal logmsg\n",
        "\tglobal saveDefault\n",
        "\ttime = datetime.datetime.now()\n",
        "\ttem = '%s: %s' % (time, msg)\n",
        "\tif save != None:\n",
        "\t\tif save:\n",
        "\t\t\tlogmsg += tem + '\\n'\n",
        "\telif saveDefault:\n",
        "\t\tlogmsg += tem + '\\n'\n",
        "\tif oneline:\n",
        "\t\tprint(tem, end='\\r')\n",
        "\telse:\n",
        "\t\tprint(tem)\n",
        "\n",
        "def marktime(marker):\n",
        "\tglobal timemark\n",
        "\ttimemark[marker] = datetime.datetime.now()\n",
        "\n",
        "def SpentTime(marker):\n",
        "\tglobal timemark\n",
        "\tif marker not in timemark:\n",
        "\t\tmsg = 'LOGGER ERROR, marker', marker, ' not found'\n",
        "\t\ttem = '%s: %s' % (time, msg)\n",
        "\t\tprint(tem)\n",
        "\t\treturn False\n",
        "\treturn datetime.datetime.now() - timemark[marker]\n",
        "\n",
        "def SpentTooLong(marker, day=0, hour=0, minute=0, second=0):\n",
        "\tglobal timemark\n",
        "\tif marker not in timemark:\n",
        "\t\tmsg = 'LOGGER ERROR, marker', marker, ' not found'\n",
        "\t\ttem = '%s: %s' % (time, msg)\n",
        "\t\tprint(tem)\n",
        "\t\treturn False\n",
        "\treturn datetime.datetime.now() - timemark[marker] >= datetime.timedelta(days=day, hours=hour, minutes=minute, seconds=second)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tlog('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq3aOYJ7dd3F",
        "outputId": "8d243c84-cb77-42e5-a529-f9a6445e9170"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-08 03:21:15.323094: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model.py"
      ],
      "metadata": {
        "id": "U_uP_M7fef9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#from Params import args\n",
        "\n",
        "torch.manual_seed(666)\n",
        "np.random.seed(666)\n",
        "\n",
        "class FC(nn.Module):\n",
        "  def __init__(self, inputdim, outputdim, bias = False):\n",
        "    super(FC, self).__init__()\n",
        "    initializer = nn.init.xavier_normal_\n",
        "    self.W_fc = nn.Parameter(initializer(torch.empty(inputdim, outputdim).cuda())) # shape latdim * latdim\n",
        "    self.bias = False\n",
        "    if bias is True:\n",
        "      initializer = nn.init.zeros_\n",
        "      self.bias_fc = nn.Parameter(initializer(torch.empty(outputdim).cuda()))\n",
        "      self.bias = bias\n",
        "\n",
        "  def forward(self, ret, act = None):\n",
        "    ret = ret @ self.W_fc\n",
        "    if self.bias is True:\n",
        "      ret = ret + self.bias_fc\n",
        "    if act == 'leakyrelu':\n",
        "      ret = torch.maximum(args.leaky * ret, ret)\n",
        "    if act == 'sigmoid':\n",
        "      ret = torch.sigmoid(ret)\n",
        "    return ret\n",
        "\n",
        "class propagate(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(propagate, self).__init__()\n",
        "    initializer = nn.init.xavier_normal_\n",
        "\n",
        "    self.fc1 = FC(args.hyperNum,args.hyperNum)\n",
        "    self.fc2 = FC(args.hyperNum,args.hyperNum)\n",
        "\n",
        "  def forward(self, V, lats, key, hyper):\n",
        "    lstlat = torch.reshape(lats[-1] @ V, [-1, args.att_head, args.latdim // args.att_head])\n",
        "    lstlat = torch.permute(lstlat, (1,2,0)) #shape head_num * (latdim/head_num) * (user num or item num)\n",
        "    temlat1 = lstlat @ key # shape head_num * (latdim/head_num) * (latdim/head_num)\n",
        "    hyper = torch.reshape(hyper, [-1, args.att_head, args.latdim // args.att_head])\n",
        "    hyper = torch.permute(hyper, (1,2,0)) #shape head_num * (latdim/head_num) * hyperNum\n",
        "    temlat1 = torch.reshape(temlat1 @ hyper, [args.latdim, -1]) #shape latdim * hyperNum\n",
        "    temlat2 = self.fc1(temlat1, act = 'leakyrelu') + temlat1 #shape latdim * hyperNum\n",
        "    temlat3 = self.fc2(temlat2, act = 'leakyrelu') + temlat2 #shape latdim * hyperNum\n",
        "\n",
        "    preNewLat = torch.reshape(torch.transpose(temlat3, 0, 1) @ V, [-1, args.att_head, args.latdim//args.att_head]) #shape hyperNum * head_num * (latdim/head_num)\n",
        "    preNewLat = torch.permute(preNewLat, (1,0,2)) #shape head num * hyperNum * latdim/head_num\n",
        "    preNewLat = hyper @ preNewLat #shape head_num * (latdim/head_num) * (latdim/head_num)\n",
        "    newLat = key @ preNewLat #shape head_num * user num or item num * (latdim/head_num)\n",
        "    newLat = torch.reshape(torch.permute(newLat,(1,0,2)),[-1,args.latdim]) #shape user num or item num * latdim\n",
        "    lats.append(newLat)\n",
        "\n",
        "class meta(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(meta, self).__init__()\n",
        "    self.fc1 = FC(args.latdim, args.latdim * args.latdim, bias = True)\n",
        "    self.fc2 = FC(args.latdim, args.latdim, bias = True)\n",
        "    #self.actFunc = nn.LeakyReLU(negative_slope=args.leaky)\n",
        "\n",
        "  def forward(self, hyper, key):\n",
        "    hyper_mean = torch.mean(hyper, dim=0, keepdim=True) #1 * latdim\n",
        "    W1 = self.fc1(hyper_mean, act = None)  # 1 * (latdim*latdim)\n",
        "    W1 = torch.reshape(W1, [args.latdim, args.latdim]) # latdim * latdim\n",
        "    b1 = self.fc2(hyper_mean, act = None) # 1 * latdim\n",
        "    ret = key @ W1 + b1\n",
        "    ret = torch.maximum(args.leaky * ret, ret) # (batchsize * latdim) * (latdim * latdim) + 1*latdim = batchsize * latdim // 534564 ?, 32\n",
        "    return ret\n",
        "\n",
        "class SHT(nn.Module):\n",
        "  def __init__(self, adj, tpadj):\n",
        "    super(SHT, self).__init__()\n",
        "\n",
        "    initializer = nn.init.xavier_normal_\n",
        "    self.adj = adj # user * item\n",
        "    self.tpadj = tpadj # item * user\n",
        "\n",
        "    self.uEmbed_ini = nn.Parameter(initializer(torch.empty(args.user,args.latdim).cuda())) #shape user * latdim\n",
        "    self.iEmbed_ini = nn.Parameter(initializer(torch.empty(args.item,args.latdim).cuda())) #shape item * latdim\n",
        "    self.uHyper = nn.Parameter(initializer(torch.empty(args.hyperNum,args.latdim).cuda())) #shape hyper num * latdim\n",
        "    self.iHyper = nn.Parameter(initializer(torch.empty(args.hyperNum,args.latdim).cuda())) #shape hyper num * latdim\n",
        "\n",
        "    #BUG! only one <K> and one <V> is needed.\n",
        "\n",
        "    self.K = nn.Parameter(initializer(torch.empty(args.latdim, args.latdim).cuda())) # shape latdim * latdim\n",
        "    self.V = nn.Parameter(initializer(torch.empty(args.latdim, args.latdim).cuda())) # shape latdim * latdim\n",
        "\n",
        "    self.user_propagate = nn.ModuleList()\n",
        "    self.item_propagate = nn.ModuleList()\n",
        "\n",
        "    self.reg = []\n",
        "\n",
        "    for i in range(args.gnn_layer):\n",
        "      self.user_propagate.append(propagate())#output : shape user num * latdim\n",
        "      self.item_propagate.append(propagate())#output : shape item num * latdim\n",
        "    \n",
        "    self.fc1_label = FC(2 * args.latdim, args.latdim, bias = True)\n",
        "    self.fc2_label = FC(args.latdim, 1, bias = True)\n",
        "\n",
        "    #BUG! only one <meta> is needed. \n",
        "    self.meta = meta()\n",
        "\n",
        "  def prepareKey(self, nodeEmbed):\n",
        "    key = torch.reshape(nodeEmbed @ self.K, [-1, args.att_head, args.latdim // args.att_head])\n",
        "    key = torch.permute(key, (1,0,2)) #shape head_num * (user num or item num) * (latdim/head_num)\n",
        "    return key\n",
        "  \n",
        "  def label(self, usrKey, itmKey, uHyper, iHyper):\n",
        "    ulat = self.meta(uHyper, usrKey) # batchsize * latdim\n",
        "    ilat = self.meta(iHyper, itmKey) # batchsize * latdim\n",
        "    lat = torch.cat([ulat, ilat], dim=-1) # batchsize * 2latdim\n",
        "    lat = self.fc1_label(lat, act = 'leakyrelu') #batchsize * latdim\n",
        "    lat = lat + ulat + ilat\n",
        "    ret = self.fc2_label(lat, act = 'sigmoid')\n",
        "    ret = torch.reshape(ret, [-1]) #降维\n",
        "    return ret\n",
        "\n",
        "  def GCN(self, ulat, ilat, adj, tpadj):\n",
        "    ulats = [ulat] #shape user * latdim\n",
        "    ilats = [ilat] #shape item * latdim\n",
        "    for i in range(args.gcn_hops):\n",
        "      temulat = torch.sparse.mm(adj,ilats[-1]) #shape user * latdim  //sparse\n",
        "      temilat = torch.sparse.mm(tpadj,ulats[-1]) #shape item * latdim  //sparse\n",
        "      ulats.append(temulat) \n",
        "      ilats.append(temilat)\n",
        "    ulats_sum = sum(ulats[1:]) #shape user * latdim\n",
        "    ilats_sum = sum(ilats[1:]) #shape item * latdim\n",
        "    return ulats_sum, ilats_sum\n",
        "\n",
        "  def Regularize(self, reg, method = 'L2'):\n",
        "    ret = 0.0\n",
        "    for i in range(len(reg)):\n",
        "      ret += torch.sum(torch.square(reg[i]))\n",
        "    return ret\n",
        "\n",
        "  def forward_test(self):\n",
        "    uEmbed_gcn, iEmbed_gcn = self.GCN(self.uEmbed_ini, self.iEmbed_ini, self.adj, self.tpadj) # usre * latdim, item * latdim\n",
        "    uEmbed0 = self.uEmbed_ini + uEmbed_gcn\n",
        "    iEmbed0 = self.iEmbed_ini + iEmbed_gcn\n",
        "\n",
        "    uKey = self.prepareKey(uEmbed0) #shape head_num * (user num) * (latdim/head_num)\n",
        "    iKey = self.prepareKey(iEmbed0) #shape head_num * (item num) * (latdim/head_num)\n",
        "\n",
        "    ulats = [uEmbed0]\n",
        "    ilats = [iEmbed0]\n",
        "    for i in range(args.gnn_layer):\n",
        "      self.user_propagate[i](self.V, ulats, uKey, self.uHyper)\n",
        "      self.item_propagate[i](self.V, ilats, iKey, self.iHyper)\n",
        "    \n",
        "    ulat = sum(ulats) #shape user * latdim\n",
        "    ilat = sum(ilats) #shape item * latdim\n",
        "    return ulat, ilat\n",
        "\n",
        "  def forward(self, uid, iid, edgeids):\n",
        "    #self.reg.extend([self.uEmbed_ini,self.iEmbed_ini,self.uHyper,self.iHyper])\n",
        "    uEmbed_gcn, iEmbed_gcn = self.GCN(self.uEmbed_ini, self.iEmbed_ini, self.adj, self.tpadj) # usre * latdim, item * latdim\n",
        "    uEmbed0 = self.uEmbed_ini + uEmbed_gcn\n",
        "    iEmbed0 = self.iEmbed_ini + iEmbed_gcn\n",
        "\n",
        "    #self.gcnNorm = (torch.sum(torch.sum(torch.square(uEmbed_gcn), dim = -1)) + torch.sum(torch.sum(torch.square(iEmbed_gcn), dim = -1))) / 2\n",
        "    #self.iniNorm = (torch.sum(torch.sum(torch.square(self.uEmbed_ini), dim = -1)) + torch.sum(torch.sum(torch.square(self.iEmbed_ini), dim = -1))) / 2\n",
        "    uKey = self.prepareKey(uEmbed0) #shape head_num * (user num) * (latdim/head_num)\n",
        "    iKey = self.prepareKey(iEmbed0) #shape head_num * (item num) * (latdim/head_num)\n",
        "    #self.reg.append(self.K)\n",
        "    ulats = [uEmbed0]\n",
        "    ilats = [iEmbed0]\n",
        "    for i in range(args.gnn_layer):\n",
        "      self.user_propagate[i](self.V, ulats, uKey, self.uHyper)\n",
        "      self.item_propagate[i](self.V, ilats, iKey, self.iHyper)\n",
        "    \n",
        "    ulat = sum(ulats) #shape user * latdim\n",
        "    ilat = sum(ilats) #shape item * latdim\n",
        "    pckUlat = torch.index_select(ulat, 0, uid.int()) #shape batch size * latdim\n",
        "    pckIlat = torch.index_select(ilat, 0, iid.int()) #shape batch size * latdim\n",
        "    preds = torch.sum(pckUlat * pckIlat, dim = -1)\n",
        "\n",
        "    idx = self.adj._indices() #shape (2, user * item)\n",
        "    \n",
        "    users, items = torch.index_select(idx[0,], 0, edgeids.int()), torch.index_select(idx[1,], 0, edgeids.int()) #shape (batchsize)\n",
        "    uKey = torch.reshape(torch.permute(uKey, (1,0,2)), [-1,args.latdim]) # user num * latdim\n",
        "    iKey = torch.reshape(torch.permute(iKey, (1,0,2)), [-1,args.latdim]) # item num * latdim\n",
        "    userKey = torch.index_select(uKey, 0, users.int())  # batchsize * latdim\n",
        "    itemKey = torch.index_select(iKey, 0, items.int())  # batchsize * latdim\n",
        "\n",
        "    scores = self.label(userKey, itemKey, self.uHyper, self.iHyper)\n",
        "    _preds = torch.sum(torch.index_select(uEmbed0, 0, users.int()) * torch.index_select(iEmbed0, 0 , items.int()), dim = -1)\n",
        "    halfNum = scores.shape[0]//2\n",
        "    fstScores = scores[:halfNum]\n",
        "    scdScores = scores[halfNum:]\n",
        "    fstPreds = _preds[:halfNum]\n",
        "    scdPreds = _preds[halfNum:]\n",
        "\n",
        "    sslLoss = torch.sum(torch.maximum(torch.Tensor([0.0]).cuda(), 1.0 - (fstPreds - scdPreds) * args.mult * (fstScores - scdScores)))\n",
        "    \n",
        "    reg = [self.uEmbed_ini,self.iEmbed_ini,self.uHyper,\\\n",
        "        self.iHyper,self.K,self.V,self.fc1_label.W_fc,\\\n",
        "        self.fc2_label.W_fc,self.meta.fc1.W_fc,\\\n",
        "        self.meta.fc2.W_fc]\n",
        "\n",
        "    return preds, sslLoss, self.Regularize(reg, method = 'L2')"
      ],
      "metadata": {
        "id": "KbdHAUl1efjX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHT.py"
      ],
      "metadata": {
        "id": "QQXJSj8oJrwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# from Model import HCCF\n",
        "# from DataHandler import DataHandler, negSamp, transToLsts, transpose\n",
        "# from Params import args\n",
        "# from TimeLogger import log\n",
        "\n",
        "class sht():\n",
        "  def __init__(self,handler):\n",
        "    self.handler = handler\n",
        "    self.handler.LoadData()\n",
        "\n",
        "    adj = handler.trnMat\n",
        "    idx, data, shape = transToLsts(adj, norm=True)\n",
        "    self.adj_py = torch.sparse.FloatTensor(idx, data, shape).to(torch.float32).cuda()\n",
        "    idx, data, shape = transToLsts(transpose(adj), norm=True)\n",
        "    self.tpAdj_py = torch.sparse.FloatTensor(idx, data, shape).to(torch.float32).cuda()\n",
        "\n",
        "    self.curepoch = 0\n",
        "    self.metrics = dict()\n",
        "    mets = ['Loss', 'preLoss', 'Recall', 'NDCG']\n",
        "    for met in mets:\n",
        "      self.metrics['Train' + met] = list()\n",
        "      self.metrics['Test' + met] = list()\n",
        "\n",
        "  def preparemodel(self):\n",
        "    self.model = SHT(self.adj_py, self.tpAdj_py).cuda()\n",
        "    self.opt = torch.optim.Adam(params = self.model.parameters(), lr=args.lr)\n",
        "    self.scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer = self.opt, gamma=args.decay)\n",
        "    print('our training parameters:')\n",
        "    for name, param in self.model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            print(name,param.shape,param.dtype)\n",
        "\n",
        "  def sampleTrainBatch(self, batIds, labelMat):\n",
        "    temLabel = labelMat[batIds].toarray()\n",
        "    batch = len(batIds)\n",
        "    temlen = batch * 2 * args.sampNum\n",
        "    uLocs = [None] * temlen\n",
        "    iLocs = [None] * temlen\n",
        "    cur = 0\n",
        "    for i in range(batch):\n",
        "      posset = np.reshape(np.argwhere(temLabel[i]!=0), [-1])\n",
        "      sampNum = min(args.sampNum, len(posset))\n",
        "      if sampNum == 0:\n",
        "        poslocs = [np.random.choice(args.item)]\n",
        "        neglocs = [poslocs[0]]\n",
        "      else:\n",
        "        poslocs = np.random.choice(posset, sampNum)\n",
        "        neglocs = negSamp(temLabel[i], sampNum, args.item)\n",
        "      for j in range(sampNum):\n",
        "        posloc = poslocs[j]\n",
        "        negloc = neglocs[j]\n",
        "        uLocs[cur] = uLocs[cur+temlen//2] = batIds[i]\n",
        "        iLocs[cur] = posloc\n",
        "        iLocs[cur+temlen//2] = negloc\n",
        "        cur += 1\n",
        "    uLocsa = uLocs[:cur] + uLocs[temlen//2: temlen//2 + cur]\n",
        "    iLocsa = iLocs[:cur] + iLocs[temlen//2: temlen//2 + cur]\n",
        "\n",
        "    edgeSampNum = int(args.edgeSampRate * args.edgeNum)\n",
        "    if edgeSampNum % 2 == 1:\n",
        "      edgeSampNum += 1\n",
        "    edgeids = np.random.choice(args.edgeNum, edgeSampNum)\n",
        "    \n",
        "    return torch.Tensor(uLocsa).cuda(), torch.Tensor(iLocsa).cuda(), torch.Tensor(edgeids).cuda()\n",
        "\n",
        "  def trainEpoch(self):\n",
        "    num = args.user\n",
        "    sfIds = np.random.permutation(num)[:args.trnNum]\n",
        "    epochLoss, epochPreLoss, epochsslLoss, epochregLoss = [0] * 4\n",
        "    num = len(sfIds)\n",
        "    steps = int(np.ceil(num / args.batch))\n",
        "\n",
        "    self.model.train()\n",
        "    for i in range(steps):\n",
        "      st = i * args.batch\n",
        "      ed = min((i+1) * args.batch, num)\n",
        "      batIds = sfIds[st: ed]\n",
        "\n",
        "      uLocs, iLocs, edgeids = self.sampleTrainBatch(batIds, self.handler.trnMat)\n",
        "      preds, sslLoss, regularize = self.model(uLocs, iLocs, edgeids)\n",
        "\n",
        "      sampNum = uLocs.shape[0] // 2\n",
        "      posPred = preds[:sampNum]\n",
        "      negPred = preds[sampNum:sampNum * 2]\n",
        "      preLoss = torch.sum(torch.maximum(torch.Tensor([0.0]).cuda(), 1.0 - (posPred - negPred))) / args.batch\n",
        "      regLoss = args.reg * regularize\n",
        "      regsslLoss = args.ssl_reg * sslLoss\n",
        "      loss = preLoss + regLoss + regsslLoss\n",
        "\n",
        "      self.opt.zero_grad()\n",
        "      loss.backward()\n",
        "      self.opt.step()\n",
        "      if i % args.decay_step == 0:\n",
        "        self.scheduler.step()\n",
        "\n",
        "      epochLoss += loss\n",
        "      epochPreLoss += preLoss\n",
        "      epochsslLoss += regsslLoss\n",
        "      epochregLoss += regLoss\n",
        "      #log('Step %d/%d: loss = %.2f, regLoss = %.2f, sslLoss = %.2f         ' % (i, steps, loss, regLoss, sslLoss), save=False, oneline=True)\n",
        "    ret = dict()\n",
        "    ret['Loss'] = epochLoss / steps\n",
        "    ret['preLoss'] = epochPreLoss / steps\n",
        "    ret['sslLoss'] = epochsslLoss / steps\n",
        "    ret['regLoss'] = epochregLoss / steps\n",
        "    return ret\n",
        "\n",
        "  def testEpoch(self):\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "      epochRecall, epochNdcg = [0] * 2\n",
        "      ids = self.handler.tstUsrs\n",
        "      num = len(ids)\n",
        "      tstBat = args.batch\n",
        "      steps = int(np.ceil(num / tstBat))\n",
        "      tstNum = 0\n",
        "      ulat, ilat = self.model.forward_test()\n",
        "      for i in range(steps):\n",
        "        st = i * tstBat\n",
        "        ed = min((i+1) * tstBat, num)\n",
        "        batIds = ids[st: ed]\n",
        "        trnPosMask = self.handler.trnMat[batIds].toarray()\n",
        "        toplocs = self.tstPred(batIds, trnPosMask, ulat, ilat)\n",
        "        recall, ndcg = self.calcRes(toplocs, self.handler.tstLocs, batIds)\n",
        "        epochRecall += recall\n",
        "        epochNdcg += ndcg\n",
        "        #log('Steps %d/%d: recall = %.2f, ndcg = %.2f          ' % (i, steps, recall, ndcg), save=False, oneline=False)\n",
        "      ret = dict()\n",
        "      ret['Recall'] = epochRecall / num\n",
        "      ret['NDCG'] = epochNdcg / num\n",
        "    return ret\n",
        "\n",
        "  def tstPred(self, batIds, trnPosMask, ulat, ilat):\n",
        "    pckUlat = torch.index_select(ulat, 0, torch.Tensor(batIds).int().cuda())\n",
        "    allPreds = pckUlat @ torch.transpose(ilat, 0, 1)\n",
        "    allPreds = allPreds.cpu().detach().numpy() * (1 - trnPosMask) - trnPosMask * 1e8\n",
        "    vals, locs = torch.topk(torch.tensor(allPreds), args.shoot)\n",
        "    return locs\n",
        "\n",
        "  def calcRes(self, topLocs, tstLocs, batIds):\n",
        "    assert topLocs.shape[0] == len(batIds)\n",
        "    allRecall = allNdcg = 0\n",
        "    recallBig = 0\n",
        "    ndcgBig =0\n",
        "    for i in range(len(batIds)):\n",
        "      temTopLocs = list(topLocs[i])\n",
        "      temTstLocs = tstLocs[batIds[i]]\n",
        "      tstNum = len(temTstLocs)\n",
        "      maxDcg = np.sum([np.reciprocal(np.log2(loc + 2)) for loc in range(min(tstNum, args.shoot))])\n",
        "      recall = dcg = 0\n",
        "      for val in temTstLocs:\n",
        "          if val in temTopLocs:\n",
        "              recall += 1\n",
        "              dcg += np.reciprocal(np.log2(temTopLocs.index(val) + 2))\n",
        "      recall = recall / tstNum\n",
        "      ndcg = dcg / maxDcg\n",
        "      allRecall += recall\n",
        "      allNdcg += ndcg\n",
        "    return allRecall, allNdcg\n",
        "\n",
        "  def loadModel(self, loadPath):\n",
        "    loadPath = loadPath\n",
        "    checkpoint = torch.load(loadPath)\n",
        "    self.model = checkpoint['model']\n",
        "    self.curepoch = checkpoint['epoch']+1\n",
        "    self.metrics = checkpoint['metrics']\n",
        "\n",
        "  def saveHistory(self):\n",
        "    savePath = r'./Model/' + args.data  + r'.pth'\n",
        "    params = {\n",
        "        'epoch' : self.curepoch,\n",
        "        'model' : self.model,\n",
        "        'metrics' : self.metrics,\n",
        "    }\n",
        "    torch.save(params, savePath)\n",
        "\n",
        "  def run(self):\n",
        "    self.preparemodel()\n",
        "    log('Model Prepared')\n",
        "    if args.load_model != None:\n",
        "      self.loadModel(args.load_model)\n",
        "      stloc = self.curepoch\n",
        "    else:\n",
        "      stloc = 0\n",
        "\n",
        "    for ep in range(stloc, args.epoch):\n",
        "      test = (ep % args.tstEpoch == 0)\n",
        "      reses = self.trainEpoch()\n",
        "      #print(self.model.hyperULat_layers[0].fc1.W_fc.weight)\n",
        "      log(self.makePrint('Train', ep, reses, test))\n",
        "      if test:\n",
        "          reses = self.testEpoch()\n",
        "          log(self.makePrint('Test', ep, reses, test))\n",
        "      if ep % args.tstEpoch == 0:\n",
        "          self.saveHistory()\n",
        "      print()\n",
        "      self.curepoch = ep\n",
        "    reses = self.testEpoch()\n",
        "    log(self.makePrint('Test', args.epoch, reses, True))\n",
        "    self.saveHistory()\n",
        "\n",
        "  def makePrint(self, name, ep, reses, save):\n",
        "    ret = 'Epoch %d/%d, %s: ' % (ep, args.epoch, name)\n",
        "    for metric in reses:\n",
        "      val = reses[metric]\n",
        "      ret += '%s = %.4f, ' % (metric, val)\n",
        "      tem = name + metric\n",
        "      if save and tem in self.metrics:\n",
        "          self.metrics[tem].append(val)\n",
        "    ret = ret[:-2] + '  '\n",
        "    return ret\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  handler = DataHandler()\n",
        "  handler.LoadData()\n",
        "  model=sht(handler)\n",
        "  model.run()"
      ],
      "metadata": {
        "id": "ISrleSUqrqLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ebaf80-50fe-43d3-9516-39864df8286c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our training parameters:\n",
            "uEmbed_ini torch.Size([29601, 32]) torch.float32\n",
            "iEmbed_ini torch.Size([24734, 32]) torch.float32\n",
            "uHyper torch.Size([128, 32]) torch.float32\n",
            "iHyper torch.Size([128, 32]) torch.float32\n",
            "K torch.Size([32, 32]) torch.float32\n",
            "V torch.Size([32, 32]) torch.float32\n",
            "user_propagate.0.fc1.W_fc torch.Size([128, 128]) torch.float32\n",
            "user_propagate.0.fc2.W_fc torch.Size([128, 128]) torch.float32\n",
            "user_propagate.1.fc1.W_fc torch.Size([128, 128]) torch.float32\n",
            "user_propagate.1.fc2.W_fc torch.Size([128, 128]) torch.float32\n",
            "item_propagate.0.fc1.W_fc torch.Size([128, 128]) torch.float32\n",
            "item_propagate.0.fc2.W_fc torch.Size([128, 128]) torch.float32\n",
            "item_propagate.1.fc1.W_fc torch.Size([128, 128]) torch.float32\n",
            "item_propagate.1.fc2.W_fc torch.Size([128, 128]) torch.float32\n",
            "fc1_label.W_fc torch.Size([64, 32]) torch.float32\n",
            "fc1_label.bias_fc torch.Size([32]) torch.float32\n",
            "fc2_label.W_fc torch.Size([32, 1]) torch.float32\n",
            "fc2_label.bias_fc torch.Size([1]) torch.float32\n",
            "meta.fc1.W_fc torch.Size([32, 1024]) torch.float32\n",
            "meta.fc1.bias_fc torch.Size([1024]) torch.float32\n",
            "meta.fc2.W_fc torch.Size([32, 32]) torch.float32\n",
            "meta.fc2.bias_fc torch.Size([32]) torch.float32\n",
            "2022-09-08 03:21:35.283985: Model Prepared\n",
            "2022-09-08 03:21:43.007008: Epoch 0/200, Train: Loss = 51.5869, preLoss = 13.7857, sslLoss = 26.7263, regLoss = 11.0749  \n",
            "2022-09-08 03:22:18.501606: Epoch 0/200, Test: Recall = 0.0249, NDCG = 0.0216  \n",
            "\n",
            "2022-09-08 03:22:26.954945: Epoch 1/200, Train: Loss = 42.1795, preLoss = 6.5675, sslLoss = 26.3356, regLoss = 9.2763  \n",
            "\n",
            "2022-09-08 03:22:34.075927: Epoch 2/200, Train: Loss = 33.0688, preLoss = 6.3758, sslLoss = 17.5972, regLoss = 9.0958  \n",
            "\n",
            "2022-09-08 03:22:41.146933: Epoch 3/200, Train: Loss = 29.0168, preLoss = 5.3809, sslLoss = 14.5250, regLoss = 9.1109  \n",
            "\n",
            "2022-09-08 03:22:48.762135: Epoch 4/200, Train: Loss = 27.7614, preLoss = 4.8831, sslLoss = 14.0831, regLoss = 8.7951  \n",
            "\n",
            "2022-09-08 03:22:55.858233: Epoch 5/200, Train: Loss = 26.7018, preLoss = 4.4644, sslLoss = 13.6600, regLoss = 8.5775  \n",
            "\n",
            "2022-09-08 03:23:02.883316: Epoch 6/200, Train: Loss = 25.4484, preLoss = 4.2164, sslLoss = 12.7349, regLoss = 8.4971  \n",
            "\n",
            "2022-09-08 03:23:09.893328: Epoch 7/200, Train: Loss = 24.1289, preLoss = 4.1069, sslLoss = 11.3884, regLoss = 8.6336  \n",
            "\n",
            "2022-09-08 03:23:16.902516: Epoch 8/200, Train: Loss = 23.1649, preLoss = 3.8305, sslLoss = 10.5717, regLoss = 8.7626  \n",
            "\n",
            "2022-09-08 03:23:23.910308: Epoch 9/200, Train: Loss = 22.5948, preLoss = 3.6892, sslLoss = 10.1081, regLoss = 8.7975  \n",
            "\n",
            "2022-09-08 03:23:31.026278: Epoch 10/200, Train: Loss = 22.2301, preLoss = 3.6297, sslLoss = 9.8448, regLoss = 8.7556  \n",
            "2022-09-08 03:24:06.064807: Epoch 10/200, Test: Recall = 0.0444, NDCG = 0.0374  \n",
            "\n",
            "2022-09-08 03:24:13.219420: Epoch 11/200, Train: Loss = 21.7954, preLoss = 3.4336, sslLoss = 9.6798, regLoss = 8.6819  \n",
            "\n",
            "2022-09-08 03:24:20.365890: Epoch 12/200, Train: Loss = 21.5848, preLoss = 3.4204, sslLoss = 9.5669, regLoss = 8.5975  \n",
            "\n",
            "2022-09-08 03:24:28.570029: Epoch 13/200, Train: Loss = 21.2052, preLoss = 3.2278, sslLoss = 9.4658, regLoss = 8.5115  \n",
            "\n",
            "2022-09-08 03:24:35.700059: Epoch 14/200, Train: Loss = 21.0057, preLoss = 3.1838, sslLoss = 9.3986, regLoss = 8.4233  \n",
            "\n",
            "2022-09-08 03:24:42.814948: Epoch 15/200, Train: Loss = 20.7391, preLoss = 3.0510, sslLoss = 9.3477, regLoss = 8.3403  \n",
            "\n",
            "2022-09-08 03:24:49.873695: Epoch 16/200, Train: Loss = 20.5576, preLoss = 2.9941, sslLoss = 9.3085, regLoss = 8.2550  \n",
            "\n",
            "2022-09-08 03:24:56.888833: Epoch 17/200, Train: Loss = 20.4403, preLoss = 2.9972, sslLoss = 9.2692, regLoss = 8.1739  \n",
            "\n",
            "2022-09-08 03:25:03.892138: Epoch 18/200, Train: Loss = 20.1876, preLoss = 2.8452, sslLoss = 9.2461, regLoss = 8.0963  \n",
            "\n",
            "2022-09-08 03:25:11.096865: Epoch 19/200, Train: Loss = 20.0394, preLoss = 2.7865, sslLoss = 9.2351, regLoss = 8.0178  \n",
            "\n",
            "2022-09-08 03:25:18.191390: Epoch 20/200, Train: Loss = 19.9242, preLoss = 2.7737, sslLoss = 9.2070, regLoss = 7.9435  \n",
            "2022-09-08 03:25:53.259477: Epoch 20/200, Test: Recall = 0.0494, NDCG = 0.0419  \n",
            "\n",
            "2022-09-08 03:26:00.557860: Epoch 21/200, Train: Loss = 19.8217, preLoss = 2.7585, sslLoss = 9.1899, regLoss = 7.8733  \n",
            "\n",
            "2022-09-08 03:26:07.618941: Epoch 22/200, Train: Loss = 19.6383, preLoss = 2.6528, sslLoss = 9.1738, regLoss = 7.8117  \n",
            "\n",
            "2022-09-08 03:26:14.668424: Epoch 23/200, Train: Loss = 19.5735, preLoss = 2.6615, sslLoss = 9.1619, regLoss = 7.7500  \n",
            "\n",
            "2022-09-08 03:26:21.688099: Epoch 24/200, Train: Loss = 19.4558, preLoss = 2.6087, sslLoss = 9.1554, regLoss = 7.6917  \n",
            "\n",
            "2022-09-08 03:26:29.833656: Epoch 25/200, Train: Loss = 19.2890, preLoss = 2.5029, sslLoss = 9.1476, regLoss = 7.6384  \n",
            "\n",
            "2022-09-08 03:26:37.031348: Epoch 26/200, Train: Loss = 19.2676, preLoss = 2.5418, sslLoss = 9.1373, regLoss = 7.5884  \n",
            "\n",
            "2022-09-08 03:26:44.064559: Epoch 27/200, Train: Loss = 19.1394, preLoss = 2.4666, sslLoss = 9.1327, regLoss = 7.5402  \n",
            "\n",
            "2022-09-08 03:26:51.155321: Epoch 28/200, Train: Loss = 19.0995, preLoss = 2.4809, sslLoss = 9.1248, regLoss = 7.4938  \n",
            "\n",
            "2022-09-08 03:26:58.149565: Epoch 29/200, Train: Loss = 18.9610, preLoss = 2.3975, sslLoss = 9.1114, regLoss = 7.4522  \n",
            "\n",
            "2022-09-08 03:27:05.149340: Epoch 30/200, Train: Loss = 18.9124, preLoss = 2.3853, sslLoss = 9.1134, regLoss = 7.4136  \n",
            "2022-09-08 03:27:40.015580: Epoch 30/200, Test: Recall = 0.0540, NDCG = 0.0455  \n",
            "\n",
            "2022-09-08 03:27:47.320314: Epoch 31/200, Train: Loss = 18.8773, preLoss = 2.4013, sslLoss = 9.1015, regLoss = 7.3746  \n",
            "\n",
            "2022-09-08 03:27:54.511874: Epoch 32/200, Train: Loss = 18.7479, preLoss = 2.3010, sslLoss = 9.1057, regLoss = 7.3412  \n",
            "\n",
            "2022-09-08 03:28:01.557541: Epoch 33/200, Train: Loss = 18.7238, preLoss = 2.3203, sslLoss = 9.0940, regLoss = 7.3094  \n",
            "\n",
            "2022-09-08 03:28:08.640317: Epoch 34/200, Train: Loss = 18.6567, preLoss = 2.2780, sslLoss = 9.1014, regLoss = 7.2773  \n",
            "\n",
            "2022-09-08 03:28:15.783626: Epoch 35/200, Train: Loss = 18.5882, preLoss = 2.2509, sslLoss = 9.0873, regLoss = 7.2500  \n",
            "\n",
            "2022-09-08 03:28:22.836111: Epoch 36/200, Train: Loss = 18.5472, preLoss = 2.2341, sslLoss = 9.0890, regLoss = 7.2240  \n",
            "\n",
            "2022-09-08 03:28:30.677509: Epoch 37/200, Train: Loss = 18.4529, preLoss = 2.1747, sslLoss = 9.0801, regLoss = 7.1981  \n",
            "\n",
            "2022-09-08 03:28:37.860682: Epoch 38/200, Train: Loss = 18.4276, preLoss = 2.1663, sslLoss = 9.0854, regLoss = 7.1760  \n",
            "\n",
            "2022-09-08 03:28:44.861957: Epoch 39/200, Train: Loss = 18.3850, preLoss = 2.1544, sslLoss = 9.0759, regLoss = 7.1547  \n",
            "\n",
            "2022-09-08 03:28:51.887910: Epoch 40/200, Train: Loss = 18.3527, preLoss = 2.1432, sslLoss = 9.0739, regLoss = 7.1357  \n",
            "2022-09-08 03:29:26.949845: Epoch 40/200, Test: Recall = 0.0576, NDCG = 0.0483  \n",
            "\n",
            "2022-09-08 03:29:34.200076: Epoch 41/200, Train: Loss = 18.3249, preLoss = 2.1361, sslLoss = 9.0722, regLoss = 7.1166  \n",
            "\n",
            "2022-09-08 03:29:41.395959: Epoch 42/200, Train: Loss = 18.2788, preLoss = 2.1065, sslLoss = 9.0734, regLoss = 7.0988  \n",
            "\n",
            "2022-09-08 03:29:48.572318: Epoch 43/200, Train: Loss = 18.2558, preLoss = 2.0994, sslLoss = 9.0725, regLoss = 7.0839  \n",
            "\n",
            "2022-09-08 03:29:55.585531: Epoch 44/200, Train: Loss = 18.2212, preLoss = 2.0777, sslLoss = 9.0745, regLoss = 7.0690  \n",
            "\n",
            "2022-09-08 03:30:02.591498: Epoch 45/200, Train: Loss = 18.1588, preLoss = 2.0377, sslLoss = 9.0659, regLoss = 7.0553  \n",
            "\n",
            "2022-09-08 03:30:09.534922: Epoch 46/200, Train: Loss = 18.1813, preLoss = 2.0702, sslLoss = 9.0688, regLoss = 7.0423  \n",
            "\n",
            "2022-09-08 03:30:16.482680: Epoch 47/200, Train: Loss = 18.1431, preLoss = 2.0410, sslLoss = 9.0719, regLoss = 7.0302  \n",
            "\n",
            "2022-09-08 03:30:23.518169: Epoch 48/200, Train: Loss = 18.1513, preLoss = 2.0622, sslLoss = 9.0694, regLoss = 7.0197  \n",
            "\n",
            "2022-09-08 03:30:30.588604: Epoch 49/200, Train: Loss = 18.0897, preLoss = 2.0142, sslLoss = 9.0660, regLoss = 7.0095  \n",
            "\n",
            "2022-09-08 03:30:38.705831: Epoch 50/200, Train: Loss = 18.1138, preLoss = 2.0458, sslLoss = 9.0671, regLoss = 7.0008  \n",
            "2022-09-08 03:31:13.494379: Epoch 50/200, Test: Recall = 0.0597, NDCG = 0.0499  \n",
            "\n",
            "2022-09-08 03:31:20.745060: Epoch 51/200, Train: Loss = 18.0663, preLoss = 2.0137, sslLoss = 9.0610, regLoss = 6.9915  \n",
            "\n",
            "2022-09-08 03:31:27.756579: Epoch 52/200, Train: Loss = 18.0732, preLoss = 2.0325, sslLoss = 9.0572, regLoss = 6.9836  \n",
            "\n",
            "2022-09-08 03:31:34.813417: Epoch 53/200, Train: Loss = 18.0259, preLoss = 1.9900, sslLoss = 9.0597, regLoss = 6.9763  \n",
            "\n",
            "2022-09-08 03:31:41.814767: Epoch 54/200, Train: Loss = 18.0103, preLoss = 1.9844, sslLoss = 9.0563, regLoss = 6.9696  \n",
            "\n",
            "2022-09-08 03:31:48.769546: Epoch 55/200, Train: Loss = 17.9830, preLoss = 1.9663, sslLoss = 9.0532, regLoss = 6.9634  \n",
            "\n",
            "2022-09-08 03:31:55.906323: Epoch 56/200, Train: Loss = 18.0064, preLoss = 1.9897, sslLoss = 9.0593, regLoss = 6.9574  \n",
            "\n",
            "2022-09-08 03:32:02.935300: Epoch 57/200, Train: Loss = 17.9388, preLoss = 1.9306, sslLoss = 9.0565, regLoss = 6.9517  \n",
            "\n",
            "2022-09-08 03:32:09.910705: Epoch 58/200, Train: Loss = 17.9796, preLoss = 1.9743, sslLoss = 9.0585, regLoss = 6.9468  \n",
            "\n",
            "2022-09-08 03:32:17.099597: Epoch 59/200, Train: Loss = 17.9799, preLoss = 1.9817, sslLoss = 9.0561, regLoss = 6.9421  \n",
            "\n",
            "2022-09-08 03:32:24.162407: Epoch 60/200, Train: Loss = 17.9514, preLoss = 1.9570, sslLoss = 9.0567, regLoss = 6.9377  \n",
            "2022-09-08 03:33:00.441738: Epoch 60/200, Test: Recall = 0.0605, NDCG = 0.0507  \n",
            "\n",
            "2022-09-08 03:33:07.728964: Epoch 61/200, Train: Loss = 17.9395, preLoss = 1.9519, sslLoss = 9.0540, regLoss = 6.9337  \n",
            "\n",
            "2022-09-08 03:33:14.818106: Epoch 62/200, Train: Loss = 17.9745, preLoss = 1.9936, sslLoss = 9.0514, regLoss = 6.9294  \n",
            "\n",
            "2022-09-08 03:33:21.860468: Epoch 63/200, Train: Loss = 17.9166, preLoss = 1.9332, sslLoss = 9.0572, regLoss = 6.9263  \n",
            "\n",
            "2022-09-08 03:33:28.991642: Epoch 64/200, Train: Loss = 17.9235, preLoss = 1.9420, sslLoss = 9.0581, regLoss = 6.9234  \n",
            "\n",
            "2022-09-08 03:33:36.019952: Epoch 65/200, Train: Loss = 17.9278, preLoss = 1.9530, sslLoss = 9.0542, regLoss = 6.9206  \n",
            "\n",
            "2022-09-08 03:33:43.031704: Epoch 66/200, Train: Loss = 17.9191, preLoss = 1.9428, sslLoss = 9.0582, regLoss = 6.9181  \n",
            "\n",
            "2022-09-08 03:33:50.123376: Epoch 67/200, Train: Loss = 17.8890, preLoss = 1.9255, sslLoss = 9.0478, regLoss = 6.9156  \n",
            "\n",
            "2022-09-08 03:33:57.191975: Epoch 68/200, Train: Loss = 17.8992, preLoss = 1.9354, sslLoss = 9.0506, regLoss = 6.9133  \n",
            "\n",
            "2022-09-08 03:34:04.176794: Epoch 69/200, Train: Loss = 17.8913, preLoss = 1.9253, sslLoss = 9.0549, regLoss = 6.9111  \n",
            "\n",
            "2022-09-08 03:34:11.235088: Epoch 70/200, Train: Loss = 17.8946, preLoss = 1.9390, sslLoss = 9.0466, regLoss = 6.9090  \n",
            "2022-09-08 03:34:47.277907: Epoch 70/200, Test: Recall = 0.0612, NDCG = 0.0511  \n",
            "\n",
            "2022-09-08 03:34:54.410278: Epoch 71/200, Train: Loss = 17.9193, preLoss = 1.9566, sslLoss = 9.0555, regLoss = 6.9071  \n",
            "\n",
            "2022-09-08 03:35:01.505043: Epoch 72/200, Train: Loss = 17.8714, preLoss = 1.9192, sslLoss = 9.0468, regLoss = 6.9055  \n",
            "\n",
            "2022-09-08 03:35:08.554829: Epoch 73/200, Train: Loss = 17.8893, preLoss = 1.9286, sslLoss = 9.0567, regLoss = 6.9039  \n",
            "\n",
            "2022-09-08 03:35:15.563283: Epoch 74/200, Train: Loss = 17.8469, preLoss = 1.8903, sslLoss = 9.0541, regLoss = 6.9025  \n",
            "\n",
            "2022-09-08 03:35:22.538409: Epoch 75/200, Train: Loss = 17.8923, preLoss = 1.9397, sslLoss = 9.0513, regLoss = 6.9013  \n",
            "\n",
            "2022-09-08 03:35:29.571282: Epoch 76/200, Train: Loss = 17.8608, preLoss = 1.9090, sslLoss = 9.0517, regLoss = 6.9001  \n",
            "\n",
            "2022-09-08 03:35:36.607725: Epoch 77/200, Train: Loss = 17.8559, preLoss = 1.9016, sslLoss = 9.0553, regLoss = 6.8990  \n",
            "\n",
            "2022-09-08 03:35:43.680502: Epoch 78/200, Train: Loss = 17.8930, preLoss = 1.9431, sslLoss = 9.0520, regLoss = 6.8979  \n",
            "\n",
            "2022-09-08 03:35:50.717482: Epoch 79/200, Train: Loss = 17.8903, preLoss = 1.9414, sslLoss = 9.0521, regLoss = 6.8969  \n",
            "\n",
            "2022-09-08 03:35:57.775212: Epoch 80/200, Train: Loss = 17.8430, preLoss = 1.8951, sslLoss = 9.0519, regLoss = 6.8960  \n",
            "2022-09-08 03:36:32.575046: Epoch 80/200, Test: Recall = 0.0617, NDCG = 0.0513  \n",
            "\n",
            "2022-09-08 03:36:39.844798: Epoch 81/200, Train: Loss = 17.8687, preLoss = 1.9188, sslLoss = 9.0547, regLoss = 6.8953  \n",
            "\n",
            "2022-09-08 03:36:48.107731: Epoch 82/200, Train: Loss = 17.8686, preLoss = 1.9251, sslLoss = 9.0490, regLoss = 6.8945  \n",
            "\n",
            "2022-09-08 03:36:55.179389: Epoch 83/200, Train: Loss = 17.8571, preLoss = 1.9091, sslLoss = 9.0542, regLoss = 6.8938  \n",
            "\n",
            "2022-09-08 03:37:02.177915: Epoch 84/200, Train: Loss = 17.9029, preLoss = 1.9545, sslLoss = 9.0553, regLoss = 6.8932  \n",
            "\n",
            "2022-09-08 03:37:09.182967: Epoch 85/200, Train: Loss = 17.8593, preLoss = 1.9117, sslLoss = 9.0550, regLoss = 6.8926  \n",
            "\n",
            "2022-09-08 03:37:16.336280: Epoch 86/200, Train: Loss = 17.8715, preLoss = 1.9278, sslLoss = 9.0516, regLoss = 6.8921  \n",
            "\n",
            "2022-09-08 03:37:23.461712: Epoch 87/200, Train: Loss = 17.8762, preLoss = 1.9310, sslLoss = 9.0536, regLoss = 6.8916  \n",
            "\n",
            "2022-09-08 03:37:30.429729: Epoch 88/200, Train: Loss = 17.8486, preLoss = 1.9041, sslLoss = 9.0534, regLoss = 6.8912  \n",
            "\n",
            "2022-09-08 03:37:37.463279: Epoch 89/200, Train: Loss = 17.8534, preLoss = 1.9065, sslLoss = 9.0562, regLoss = 6.8907  \n",
            "\n",
            "2022-09-08 03:37:44.447302: Epoch 90/200, Train: Loss = 17.8687, preLoss = 1.9257, sslLoss = 9.0527, regLoss = 6.8904  \n",
            "2022-09-08 03:38:19.205753: Epoch 90/200, Test: Recall = 0.0620, NDCG = 0.0515  \n",
            "\n",
            "2022-09-08 03:38:26.478595: Epoch 91/200, Train: Loss = 17.8691, preLoss = 1.9266, sslLoss = 9.0525, regLoss = 6.8900  \n",
            "\n",
            "2022-09-08 03:38:33.676251: Epoch 92/200, Train: Loss = 17.9055, preLoss = 1.9554, sslLoss = 9.0604, regLoss = 6.8896  \n",
            "\n",
            "2022-09-08 03:38:40.875352: Epoch 93/200, Train: Loss = 17.8571, preLoss = 1.9177, sslLoss = 9.0501, regLoss = 6.8893  \n",
            "\n",
            "2022-09-08 03:38:49.067512: Epoch 94/200, Train: Loss = 17.8375, preLoss = 1.9001, sslLoss = 9.0483, regLoss = 6.8890  \n",
            "\n",
            "2022-09-08 03:38:56.289981: Epoch 95/200, Train: Loss = 17.8380, preLoss = 1.8943, sslLoss = 9.0549, regLoss = 6.8888  \n",
            "\n",
            "2022-09-08 03:39:03.440709: Epoch 96/200, Train: Loss = 17.8679, preLoss = 1.9247, sslLoss = 9.0547, regLoss = 6.8886  \n",
            "\n",
            "2022-09-08 03:39:10.637441: Epoch 97/200, Train: Loss = 17.9194, preLoss = 1.9775, sslLoss = 9.0535, regLoss = 6.8883  \n",
            "\n",
            "2022-09-08 03:39:17.700975: Epoch 98/200, Train: Loss = 17.8459, preLoss = 1.9121, sslLoss = 9.0457, regLoss = 6.8881  \n",
            "\n",
            "2022-09-08 03:39:24.843047: Epoch 99/200, Train: Loss = 17.8500, preLoss = 1.9129, sslLoss = 9.0493, regLoss = 6.8879  \n",
            "\n",
            "2022-09-08 03:39:31.850918: Epoch 100/200, Train: Loss = 17.8553, preLoss = 1.9204, sslLoss = 9.0473, regLoss = 6.8877  \n",
            "2022-09-08 03:40:06.723477: Epoch 100/200, Test: Recall = 0.0618, NDCG = 0.0515  \n",
            "\n",
            "2022-09-08 03:40:14.139384: Epoch 101/200, Train: Loss = 17.8374, preLoss = 1.9022, sslLoss = 9.0476, regLoss = 6.8875  \n",
            "\n",
            "2022-09-08 03:40:21.270313: Epoch 102/200, Train: Loss = 17.8367, preLoss = 1.8974, sslLoss = 9.0519, regLoss = 6.8874  \n",
            "\n",
            "2022-09-08 03:40:28.455514: Epoch 103/200, Train: Loss = 17.8540, preLoss = 1.9198, sslLoss = 9.0469, regLoss = 6.8873  \n",
            "\n",
            "2022-09-08 03:40:35.665951: Epoch 104/200, Train: Loss = 17.8799, preLoss = 1.9319, sslLoss = 9.0608, regLoss = 6.8871  \n",
            "\n",
            "2022-09-08 03:40:42.863894: Epoch 105/200, Train: Loss = 17.8412, preLoss = 1.9044, sslLoss = 9.0498, regLoss = 6.8870  \n",
            "\n",
            "2022-09-08 03:40:51.063416: Epoch 106/200, Train: Loss = 17.8387, preLoss = 1.8963, sslLoss = 9.0555, regLoss = 6.8869  \n",
            "\n",
            "2022-09-08 03:40:58.273859: Epoch 107/200, Train: Loss = 17.8276, preLoss = 1.8858, sslLoss = 9.0549, regLoss = 6.8868  \n",
            "\n",
            "2022-09-08 03:41:05.419319: Epoch 108/200, Train: Loss = 17.8409, preLoss = 1.9093, sslLoss = 9.0448, regLoss = 6.8868  \n",
            "\n",
            "2022-09-08 03:41:12.455496: Epoch 109/200, Train: Loss = 17.8757, preLoss = 1.9356, sslLoss = 9.0535, regLoss = 6.8867  \n",
            "\n",
            "2022-09-08 03:41:19.561500: Epoch 110/200, Train: Loss = 17.8533, preLoss = 1.9162, sslLoss = 9.0506, regLoss = 6.8866  \n",
            "2022-09-08 03:41:54.659813: Epoch 110/200, Test: Recall = 0.0618, NDCG = 0.0515  \n",
            "\n",
            "2022-09-08 03:42:01.848462: Epoch 111/200, Train: Loss = 17.8335, preLoss = 1.8991, sslLoss = 9.0480, regLoss = 6.8865  \n",
            "\n",
            "2022-09-08 03:42:09.073123: Epoch 112/200, Train: Loss = 17.8472, preLoss = 1.9069, sslLoss = 9.0539, regLoss = 6.8864  \n",
            "\n",
            "2022-09-08 03:42:16.248908: Epoch 113/200, Train: Loss = 17.8508, preLoss = 1.9040, sslLoss = 9.0604, regLoss = 6.8864  \n",
            "\n",
            "2022-09-08 03:42:23.387141: Epoch 114/200, Train: Loss = 17.7930, preLoss = 1.8602, sslLoss = 9.0465, regLoss = 6.8863  \n",
            "\n",
            "2022-09-08 03:42:30.425651: Epoch 115/200, Train: Loss = 17.8460, preLoss = 1.9080, sslLoss = 9.0517, regLoss = 6.8863  \n",
            "\n",
            "2022-09-08 03:42:37.424459: Epoch 116/200, Train: Loss = 17.8666, preLoss = 1.9268, sslLoss = 9.0536, regLoss = 6.8862  \n",
            "\n",
            "2022-09-08 03:42:44.507281: Epoch 117/200, Train: Loss = 17.8919, preLoss = 1.9535, sslLoss = 9.0523, regLoss = 6.8862  \n",
            "\n",
            "2022-09-08 03:42:51.981566: Epoch 118/200, Train: Loss = 17.8715, preLoss = 1.9270, sslLoss = 9.0584, regLoss = 6.8861  \n",
            "\n",
            "2022-09-08 03:42:59.713243: Epoch 119/200, Train: Loss = 17.8650, preLoss = 1.9285, sslLoss = 9.0504, regLoss = 6.8861  \n",
            "\n",
            "2022-09-08 03:43:06.712256: Epoch 120/200, Train: Loss = 17.8301, preLoss = 1.8906, sslLoss = 9.0534, regLoss = 6.8861  \n",
            "2022-09-08 03:43:41.737951: Epoch 120/200, Test: Recall = 0.0619, NDCG = 0.0515  \n",
            "\n",
            "2022-09-08 03:43:49.036747: Epoch 121/200, Train: Loss = 17.8291, preLoss = 1.8860, sslLoss = 9.0571, regLoss = 6.8860  \n",
            "\n",
            "2022-09-08 03:43:56.169828: Epoch 122/200, Train: Loss = 17.8751, preLoss = 1.9408, sslLoss = 9.0483, regLoss = 6.8860  \n",
            "\n",
            "2022-09-08 03:44:03.131083: Epoch 123/200, Train: Loss = 17.8237, preLoss = 1.8856, sslLoss = 9.0521, regLoss = 6.8860  \n",
            "\n",
            "2022-09-08 03:44:10.087253: Epoch 124/200, Train: Loss = 17.8396, preLoss = 1.8964, sslLoss = 9.0572, regLoss = 6.8859  \n",
            "\n",
            "2022-09-08 03:44:17.181735: Epoch 125/200, Train: Loss = 17.8559, preLoss = 1.9218, sslLoss = 9.0481, regLoss = 6.8859  \n",
            "\n",
            "2022-09-08 03:44:24.290786: Epoch 126/200, Train: Loss = 17.8150, preLoss = 1.8805, sslLoss = 9.0486, regLoss = 6.8859  \n",
            "\n",
            "2022-09-08 03:44:31.438780: Epoch 127/200, Train: Loss = 17.8474, preLoss = 1.9076, sslLoss = 9.0540, regLoss = 6.8859  \n",
            "\n",
            "2022-09-08 03:44:38.420122: Epoch 128/200, Train: Loss = 17.8007, preLoss = 1.8651, sslLoss = 9.0497, regLoss = 6.8859  \n",
            "\n",
            "2022-09-08 03:44:45.446935: Epoch 129/200, Train: Loss = 17.8419, preLoss = 1.9003, sslLoss = 9.0557, regLoss = 6.8858  \n",
            "\n",
            "2022-09-08 03:44:52.439517: Epoch 130/200, Train: Loss = 17.8536, preLoss = 1.9104, sslLoss = 9.0574, regLoss = 6.8858  \n",
            "2022-09-08 03:45:28.653518: Epoch 130/200, Test: Recall = 0.0619, NDCG = 0.0516  \n",
            "\n",
            "2022-09-08 03:45:35.880993: Epoch 131/200, Train: Loss = 17.7936, preLoss = 1.8601, sslLoss = 9.0477, regLoss = 6.8858  \n",
            "\n",
            "2022-09-08 03:45:42.974946: Epoch 132/200, Train: Loss = 17.8576, preLoss = 1.9184, sslLoss = 9.0534, regLoss = 6.8858  \n",
            "\n",
            "2022-09-08 03:45:49.932477: Epoch 133/200, Train: Loss = 17.9060, preLoss = 1.9716, sslLoss = 9.0486, regLoss = 6.8858  \n",
            "\n",
            "2022-09-08 03:45:57.001416: Epoch 134/200, Train: Loss = 17.8198, preLoss = 1.8760, sslLoss = 9.0580, regLoss = 6.8858  \n",
            "\n",
            "2022-09-08 03:46:04.064562: Epoch 135/200, Train: Loss = 17.8369, preLoss = 1.9024, sslLoss = 9.0487, regLoss = 6.8858  \n",
            "\n",
            "2022-09-08 03:46:11.158118: Epoch 136/200, Train: Loss = 17.8478, preLoss = 1.9105, sslLoss = 9.0515, regLoss = 6.8858  \n",
            "\n",
            "2022-09-08 03:46:18.167063: Epoch 137/200, Train: Loss = 17.8059, preLoss = 1.8638, sslLoss = 9.0564, regLoss = 6.8858  \n",
            "\n",
            "2022-09-08 03:46:25.135587: Epoch 138/200, Train: Loss = 17.8730, preLoss = 1.9260, sslLoss = 9.0613, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:46:32.129272: Epoch 139/200, Train: Loss = 17.8745, preLoss = 1.9375, sslLoss = 9.0513, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:46:39.198511: Epoch 140/200, Train: Loss = 17.8510, preLoss = 1.9153, sslLoss = 9.0499, regLoss = 6.8857  \n",
            "2022-09-08 03:47:15.219648: Epoch 140/200, Test: Recall = 0.0619, NDCG = 0.0516  \n",
            "\n",
            "2022-09-08 03:47:22.416684: Epoch 141/200, Train: Loss = 17.8514, preLoss = 1.9142, sslLoss = 9.0515, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:47:29.468055: Epoch 142/200, Train: Loss = 17.8426, preLoss = 1.9055, sslLoss = 9.0514, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:47:36.555511: Epoch 143/200, Train: Loss = 17.8480, preLoss = 1.9159, sslLoss = 9.0463, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:47:43.641740: Epoch 144/200, Train: Loss = 17.8894, preLoss = 1.9547, sslLoss = 9.0490, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:47:50.647118: Epoch 145/200, Train: Loss = 17.8106, preLoss = 1.8769, sslLoss = 9.0480, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:47:57.725820: Epoch 146/200, Train: Loss = 17.8437, preLoss = 1.9035, sslLoss = 9.0545, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:48:04.715013: Epoch 147/200, Train: Loss = 17.8258, preLoss = 1.8891, sslLoss = 9.0510, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:48:11.695218: Epoch 148/200, Train: Loss = 17.8663, preLoss = 1.9306, sslLoss = 9.0500, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:48:18.754748: Epoch 149/200, Train: Loss = 17.8210, preLoss = 1.8875, sslLoss = 9.0477, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:48:25.779260: Epoch 150/200, Train: Loss = 17.8255, preLoss = 1.8884, sslLoss = 9.0514, regLoss = 6.8857  \n",
            "2022-09-08 03:49:00.749635: Epoch 150/200, Test: Recall = 0.0619, NDCG = 0.0516  \n",
            "\n",
            "2022-09-08 03:49:09.046014: Epoch 151/200, Train: Loss = 17.8344, preLoss = 1.8963, sslLoss = 9.0523, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:49:16.177325: Epoch 152/200, Train: Loss = 17.8649, preLoss = 1.9249, sslLoss = 9.0543, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:49:23.237504: Epoch 153/200, Train: Loss = 17.8781, preLoss = 1.9391, sslLoss = 9.0533, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:49:30.352113: Epoch 154/200, Train: Loss = 17.8579, preLoss = 1.9224, sslLoss = 9.0497, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:49:37.498685: Epoch 155/200, Train: Loss = 17.8312, preLoss = 1.8933, sslLoss = 9.0521, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:49:44.500084: Epoch 156/200, Train: Loss = 17.8358, preLoss = 1.9005, sslLoss = 9.0496, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:49:51.533821: Epoch 157/200, Train: Loss = 17.8675, preLoss = 1.9296, sslLoss = 9.0521, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:49:58.782993: Epoch 158/200, Train: Loss = 17.7836, preLoss = 1.8528, sslLoss = 9.0451, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:50:05.868045: Epoch 159/200, Train: Loss = 17.8235, preLoss = 1.8870, sslLoss = 9.0508, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:50:12.790958: Epoch 160/200, Train: Loss = 17.8375, preLoss = 1.9026, sslLoss = 9.0491, regLoss = 6.8857  \n",
            "2022-09-08 03:50:47.707932: Epoch 160/200, Test: Recall = 0.0619, NDCG = 0.0516  \n",
            "\n",
            "2022-09-08 03:50:54.966152: Epoch 161/200, Train: Loss = 17.8777, preLoss = 1.9400, sslLoss = 9.0520, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:51:02.158511: Epoch 162/200, Train: Loss = 17.8230, preLoss = 1.8887, sslLoss = 9.0486, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:51:10.341206: Epoch 163/200, Train: Loss = 17.7992, preLoss = 1.8673, sslLoss = 9.0462, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:51:17.533125: Epoch 164/200, Train: Loss = 17.8383, preLoss = 1.9145, sslLoss = 9.0380, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:51:24.717140: Epoch 165/200, Train: Loss = 17.8459, preLoss = 1.9124, sslLoss = 9.0479, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:51:31.896672: Epoch 166/200, Train: Loss = 17.8755, preLoss = 1.9455, sslLoss = 9.0442, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:51:39.033502: Epoch 167/200, Train: Loss = 17.8246, preLoss = 1.8856, sslLoss = 9.0533, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:51:46.044915: Epoch 168/200, Train: Loss = 17.8614, preLoss = 1.9283, sslLoss = 9.0474, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:51:53.060272: Epoch 169/200, Train: Loss = 17.8381, preLoss = 1.9029, sslLoss = 9.0495, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:52:00.041004: Epoch 170/200, Train: Loss = 17.8452, preLoss = 1.9104, sslLoss = 9.0491, regLoss = 6.8857  \n",
            "2022-09-08 03:52:34.892079: Epoch 170/200, Test: Recall = 0.0619, NDCG = 0.0516  \n",
            "\n",
            "2022-09-08 03:52:42.194638: Epoch 171/200, Train: Loss = 17.8146, preLoss = 1.8737, sslLoss = 9.0553, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:52:49.309734: Epoch 172/200, Train: Loss = 17.8252, preLoss = 1.8920, sslLoss = 9.0476, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:52:56.399688: Epoch 173/200, Train: Loss = 17.8568, preLoss = 1.9199, sslLoss = 9.0513, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:53:03.503332: Epoch 174/200, Train: Loss = 17.8466, preLoss = 1.9089, sslLoss = 9.0521, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:53:11.724762: Epoch 175/200, Train: Loss = 17.8492, preLoss = 1.9095, sslLoss = 9.0540, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:53:18.922971: Epoch 176/200, Train: Loss = 17.8339, preLoss = 1.9045, sslLoss = 9.0436, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:53:26.069012: Epoch 177/200, Train: Loss = 17.8133, preLoss = 1.8715, sslLoss = 9.0560, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:53:33.254595: Epoch 178/200, Train: Loss = 17.8688, preLoss = 1.9260, sslLoss = 9.0571, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:53:40.331203: Epoch 179/200, Train: Loss = 17.8167, preLoss = 1.8853, sslLoss = 9.0457, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:53:47.424229: Epoch 180/200, Train: Loss = 17.8189, preLoss = 1.8808, sslLoss = 9.0524, regLoss = 6.8857  \n",
            "2022-09-08 03:54:22.494852: Epoch 180/200, Test: Recall = 0.0619, NDCG = 0.0516  \n",
            "\n",
            "2022-09-08 03:54:29.729800: Epoch 181/200, Train: Loss = 17.8091, preLoss = 1.8735, sslLoss = 9.0500, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:54:36.852866: Epoch 182/200, Train: Loss = 17.8578, preLoss = 1.9190, sslLoss = 9.0531, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:54:43.879224: Epoch 183/200, Train: Loss = 17.8154, preLoss = 1.8782, sslLoss = 9.0515, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:54:50.981806: Epoch 184/200, Train: Loss = 17.8012, preLoss = 1.8708, sslLoss = 9.0447, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:54:58.032631: Epoch 185/200, Train: Loss = 17.8654, preLoss = 1.9338, sslLoss = 9.0459, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:55:05.017780: Epoch 186/200, Train: Loss = 17.8438, preLoss = 1.9052, sslLoss = 9.0529, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:55:12.723974: Epoch 187/200, Train: Loss = 17.8405, preLoss = 1.8970, sslLoss = 9.0578, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:55:20.462677: Epoch 188/200, Train: Loss = 17.8604, preLoss = 1.9252, sslLoss = 9.0495, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:55:27.466130: Epoch 189/200, Train: Loss = 17.8208, preLoss = 1.8893, sslLoss = 9.0459, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:55:34.540879: Epoch 190/200, Train: Loss = 17.8825, preLoss = 1.9496, sslLoss = 9.0472, regLoss = 6.8857  \n",
            "2022-09-08 03:56:09.603929: Epoch 190/200, Test: Recall = 0.0619, NDCG = 0.0516  \n",
            "\n",
            "2022-09-08 03:56:16.946288: Epoch 191/200, Train: Loss = 17.8288, preLoss = 1.8929, sslLoss = 9.0503, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:56:24.090610: Epoch 192/200, Train: Loss = 17.8432, preLoss = 1.9139, sslLoss = 9.0437, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:56:31.202932: Epoch 193/200, Train: Loss = 17.8174, preLoss = 1.8808, sslLoss = 9.0509, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:56:38.286624: Epoch 194/200, Train: Loss = 17.8554, preLoss = 1.9226, sslLoss = 9.0471, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:56:45.298108: Epoch 195/200, Train: Loss = 17.8219, preLoss = 1.8838, sslLoss = 9.0524, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:56:52.425503: Epoch 196/200, Train: Loss = 17.8458, preLoss = 1.9129, sslLoss = 9.0472, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:56:59.563850: Epoch 197/200, Train: Loss = 17.8564, preLoss = 1.9172, sslLoss = 9.0535, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:57:06.645452: Epoch 198/200, Train: Loss = 17.8412, preLoss = 1.9097, sslLoss = 9.0458, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:57:14.048170: Epoch 199/200, Train: Loss = 17.8580, preLoss = 1.9189, sslLoss = 9.0534, regLoss = 6.8857  \n",
            "\n",
            "2022-09-08 03:57:49.788808: Epoch 200/200, Test: Recall = 0.0619, NDCG = 0.0516  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# draft"
      ],
      "metadata": {
        "id": "dlOWjvGXuWlr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KgUrxX5Eem6P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# handler = DataHandler()\n",
        "# handler.LoadData()\n",
        "# adj = handler.trnMat\n",
        "# idx, data, shape = transToLsts(adj, norm=True)\n",
        "# adjp = torch.sparse.FloatTensor(idx, data, shape).to(torch.float32).to(args.device)\n",
        "# idx, data, shape = transToLsts(transpose(adj), norm=True)\n",
        "# tpadjp = torch.sparse.FloatTensor(idx, data, shape).to(torch.float32).to(args.device)\n",
        "# model = SHT(adjp, tpadjp).to(args.device)\n",
        "# opt = torch.optim.Adam(params = model.parameters(), lr=args.lr)\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer = opt, gamma=args.decay)\n",
        "# for name, param in model.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         print(name,param.shape,param.dtype)\n",
        "        \n",
        "# def sampleTrainBatch(batIds, labelMat):\n",
        "#   temLabel = labelMat[batIds].toarray()\n",
        "#   batch = len(batIds)\n",
        "#   temlen = batch * 2 * args.sampNum\n",
        "#   uLocs = [None] * temlen\n",
        "#   iLocs = [None] * temlen\n",
        "#   cur = 0\n",
        "#   for i in range(batch):\n",
        "#       posset = np.reshape(np.argwhere(temLabel[i]!=0), [-1])\n",
        "#       sampNum = min(args.sampNum, len(posset))\n",
        "#       if sampNum == 0:\n",
        "#           poslocs = [np.random.choice(args.item)]\n",
        "#           neglocs = [poslocs[0]]\n",
        "#       else:\n",
        "#           poslocs = np.random.choice(posset, sampNum)\n",
        "#           neglocs = negSamp(temLabel[i], sampNum, args.item)\n",
        "#       for j in range(sampNum):\n",
        "#           posloc = poslocs[j]\n",
        "#           negloc = neglocs[j]\n",
        "#           uLocs[cur] = uLocs[cur+temlen//2] = batIds[i]\n",
        "#           iLocs[cur] = posloc\n",
        "#           iLocs[cur+temlen//2] = negloc\n",
        "#           cur += 1\n",
        "#   uLocsa = uLocs[:cur] + uLocs[temlen//2: temlen//2 + cur]\n",
        "#   iLocsa = iLocs[:cur] + iLocs[temlen//2: temlen//2 + cur]\n",
        "\n",
        "#   edgeSampNum = int(args.edgeSampRate * args.edgeNum)\n",
        "#   if edgeSampNum % 2 == 1:\n",
        "#     edgeSampNum += 1\n",
        "#   edgeids = np.random.choice(args.edgeNum, edgeSampNum)\n",
        "  \n",
        "#   return torch.Tensor(uLocsa).cuda(), torch.Tensor(iLocsa).cuda(), torch.Tensor(edgeids).cuda()\n",
        "# def trainEpoch():\n",
        "#   num = args.user\n",
        "#   sfIds = np.random.permutation(num)[:args.trnNum]\n",
        "#   epochLoss, epochPreLoss, epochsslLoss, epochregLoss = [0] * 4\n",
        "#   num = len(sfIds)\n",
        "#   steps = int(np.ceil(num / args.batch))\n",
        "\n",
        "#   model.train()\n",
        "#   for i in range(steps):\n",
        "#     st = i * args.batch\n",
        "#     ed = min((i+1) * args.batch, num)\n",
        "#     batIds = sfIds[st: ed]\n",
        "\n",
        "#     uLocs, iLocs, edgeids = sampleTrainBatch(batIds, handler.trnMat)\n",
        "#     preds, sslLoss, regularize = model(uLocs, iLocs, edgeids)\n",
        "\n",
        "#     sampNum = uLocs.shape[0] // 2\n",
        "#     posPred = preds[:sampNum]\n",
        "#     negPred = preds[sampNum:sampNum * 2]\n",
        "#     preLoss = torch.sum(torch.maximum(torch.Tensor([0.0]).cuda(), 1.0 - (posPred - negPred))) / args.batch\n",
        "#     regLoss = args.reg * regularize\n",
        "#     regsslLoss = args.ssl_reg * sslLoss\n",
        "#     loss = preLoss + regLoss + regsslLoss\n",
        "\n",
        "#     opt.zero_grad()\n",
        "#     loss.backward()\n",
        "#     opt.step()\n",
        "#     if i % args.decay_step == 0:\n",
        "#       scheduler.step()\n",
        "\n",
        "#     epochLoss += loss\n",
        "#     epochPreLoss += preLoss\n",
        "#     epochsslLoss += regsslLoss\n",
        "#     epochregLoss += regLoss\n",
        "#     #log('Step %d/%d: loss = %.2f, regLoss = %.2f, sslLoss = %.2f         ' % (i, steps, loss, regLoss, sslLoss), save=False, oneline=True)\n",
        "#   ret = dict()\n",
        "#   ret['Loss'] = epochLoss / steps\n",
        "#   ret['preLoss'] = epochPreLoss / steps\n",
        "#   ret['sslLoss'] = epochsslLoss / steps\n",
        "#   ret['regLoss'] = epochregLoss / steps\n",
        "#   return ret\n",
        "# def calcRes(topLocs, tstLocs, batIds):\n",
        "#   assert topLocs.shape[0] == len(batIds)\n",
        "#   allRecall = allNdcg = 0\n",
        "#   recallBig = 0\n",
        "#   ndcgBig =0\n",
        "#   for i in range(len(batIds)):\n",
        "#     temTopLocs = list(topLocs[i])\n",
        "#     temTstLocs = tstLocs[batIds[i]]\n",
        "#     tstNum = len(temTstLocs)\n",
        "#     maxDcg = np.sum([np.reciprocal(np.log2(loc + 2)) for loc in range(min(tstNum, args.shoot))])\n",
        "#     recall = dcg = 0\n",
        "#     for val in temTstLocs:\n",
        "#       if val in temTopLocs:\n",
        "#         recall += 1\n",
        "#         dcg += np.reciprocal(np.log2(temTopLocs.index(val) + 2))\n",
        "#     recall = recall / tstNum\n",
        "#     ndcg = dcg / maxDcg\n",
        "#     allRecall += recall\n",
        "#     allNdcg += ndcg\n",
        "#   return allRecall, allNdcg\n",
        "\n",
        "# def tstPred(batIds, trnPosMask, ulat, ilat):\n",
        "#   pckUlat = torch.index_select(ulat, 0, torch.Tensor(batIds).int().cuda())\n",
        "#   allPreds = pckUlat @ torch.transpose(ilat, 0, 1)\n",
        "#   allPreds = allPreds.cpu().detach().numpy() * (1 - trnPosMask) - trnPosMask * 1e8\n",
        "#   vals, locs = torch.topk(torch.tensor(allPreds), args.shoot)\n",
        "#   return locs\n",
        "\n",
        "# def testEpoch():\n",
        "#   model.eval()\n",
        "#   with torch.no_grad():\n",
        "#     epochRecall, epochNdcg = [0] * 2\n",
        "#     ids = handler.tstUsrs\n",
        "#     num = len(ids)\n",
        "#     tstBat = args.batch\n",
        "#     steps = int(np.ceil(num / tstBat))\n",
        "#     tstNum = 0\n",
        "#     ulat, ilat = model.forward_test()\n",
        "#     for i in range(steps):\n",
        "#         st = i * tstBat\n",
        "#         ed = min((i+1) * tstBat, num)\n",
        "#         batIds = ids[st: ed]\n",
        "#         trnPosMask = handler.trnMat[batIds].toarray()\n",
        "#         toplocs = tstPred(batIds, trnPosMask, ulat, ilat)\n",
        "#         recall, ndcg = calcRes(toplocs, handler.tstLocs, batIds)\n",
        "#         epochRecall += recall\n",
        "#         epochNdcg += ndcg\n",
        "#         #log('Steps %d/%d: recall = %.2f, ndcg = %.2f          ' % (i, steps, recall, ndcg), save=False, oneline=False)\n",
        "#     ret = dict()\n",
        "#     ret['Recall'] = epochRecall / num\n",
        "#     ret['NDCG'] = epochNdcg / num\n",
        "#   return ret\n",
        "# def loadModel(loadPath):\n",
        "#     loadPath = loadPath\n",
        "#     checkpoint = torch.load(loadPath)\n",
        "#     model = checkpoint['model']\n",
        "#     #curepoch = checkpoint['epoch']+1\n",
        "#     # self.ulat = checkpoint['ulat']\n",
        "#     # self.ilat = checkpoint['ilat']\n",
        "#     #metrics = checkpoint['metrics']\n",
        "\n",
        "# def saveHistory():\n",
        "\n",
        "#     savePath = r'./Model/' + args.data  + r'.pth'\n",
        "#     params = {\n",
        "#         #'epoch' : curepoch,\n",
        "#         'model' : model,\n",
        "#         # 'ulat' : self.ulat,\n",
        "#         # 'ilat' : self.ilat,\n",
        "#         #'metrics' : metrics,\n",
        "#     }\n",
        "#     torch.save(params, savePath)\n",
        "\n",
        "# def makePrint(name, ep, reses, save):\n",
        "#   ret = 'Epoch %d/%d, %s: ' % (ep, args.epoch, name)\n",
        "#   for metric in reses:\n",
        "#     val = reses[metric]\n",
        "#     ret += '%s = %.4f, ' % (metric, val)\n",
        "#     tem = name + metric\n",
        "#     # if save and tem in self.metrics:\n",
        "#     #   self.metrics[tem].append(val)\n",
        "#   ret = ret[:-2] + '  '\n",
        "#   return ret\n",
        "# def run():\n",
        "#   #preparemodel()\n",
        "#   log('Model Prepared')\n",
        "#   if args.load_model != None:\n",
        "#     loadModel(args.load_model)\n",
        "#     #stloc = self.curepoch\n",
        "#   else:\n",
        "#     stloc = 0\n",
        "\n",
        "#   for ep in range(stloc, args.epoch):\n",
        "#     test = (ep % args.tstEpoch == 0)\n",
        "#     reses = trainEpoch()\n",
        "#     #print(self.model.hyperULat_layers[0].fc1.W_fc.weight)\n",
        "#     log(makePrint('Train', ep, reses, test))\n",
        "#     if test:\n",
        "#       reses = testEpoch()\n",
        "#       log(makePrint('Test', ep, reses, test))\n",
        "#     if ep % args.tstEpoch == 0:\n",
        "#       saveHistory()\n",
        "#     print()\n",
        "#     curepoch = ep\n",
        "#   reses = testEpoch()\n",
        "#   log(makePrint('Test', args.epoch, reses, True))\n",
        "#   saveHistory()\n",
        "  \n",
        "# run()"
      ],
      "metadata": {
        "id": "CssAcn4Sem3v"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}